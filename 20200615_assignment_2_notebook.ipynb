{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b75a6390",
   "metadata": {},
   "source": [
    "## Importing relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539d7341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9489e08",
   "metadata": {},
   "source": [
    "## Importing dataset - replacing nan placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d40e43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "penguins = pd.read_csv('PenguinsMV0.2.csv', index_col = 0,na_values = '?')\n",
    "print(penguins.shape)\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714c873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Breaking dataset into X and y columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88322fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = penguins.drop(columns=['species']).values\n",
    "y = penguins['species']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b07ce8",
   "metadata": {},
   "source": [
    "### Custom Gaussin NB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b0bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_GB_3(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Custom Gaussian Naive Bayes classifier, can handle NaN values in training dataset as well as in predict queries.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dfs = {}\n",
    "        self.prior_probabilities = {}\n",
    "        self.std_mean_dict= {}\n",
    "        self.feature_count = -1\n",
    "        \n",
    "    def shape_checker(self, input_array):\n",
    "        \n",
    "        it = iter(input_array)\n",
    "        the_len = len(next(it))\n",
    "        if not all(len(l) == the_len for l in it):\n",
    "             raise ValueError('All list items must contain the same number of features')\n",
    "    \n",
    "    def fit(self, Xt, yt):\n",
    "        \n",
    "        \n",
    "        def nan_std_mean_calculator():\n",
    "            \n",
    "            #have std and mean for nan cleaned transposed arrays\n",
    "            for key, value in self.dfs.items():\n",
    "                self.std_mean_dict[key] = {}\n",
    "\n",
    "                for column in self.dfs[key]:\n",
    "                    \n",
    "                    numpystd = np.nanstd(self.dfs[key][column].values)\n",
    "                    \n",
    "                    numpymean = np.nanmean(self.dfs[key][column].values)\n",
    "                    \n",
    "                    self.std_mean_dict[key][column] = {\"std\" : numpystd, \"mean\": numpymean}\n",
    "\n",
    "                    \n",
    "        \n",
    "   \n",
    "        def prior_probability_generator(target_array):\n",
    "            \"\"\"\n",
    "                Generates dictionary keys based on target values and values being the prior probabilities of of target values\n",
    "                return value is said dictionary\n",
    "            \"\"\"\n",
    "            return dict(zip(target_array.value_counts(normalize=True).index,target_array.value_counts(normalize=True).values))\n",
    "\n",
    "        def data_splitter(full_data, target_feature):\n",
    "            \"\"\"\n",
    "            Populates a dictionary with key values based on target feature values\n",
    "            Returns said dictionary\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            dfs = {}\n",
    "\n",
    "            for value in full_data[target_feature].value_counts().index.tolist():\n",
    "                    dfs[value] = full_data[full_data[target_feature]== value].drop(columns=[target_feature])\n",
    "\n",
    "            return dfs\n",
    "        \n",
    "        \n",
    "        \n",
    "        #checking the shape of the input is correct\n",
    "        self.shape_checker(Xt)\n",
    "        \n",
    "        #Xt and Yt simply represent the feature array and target features array respectively\n",
    "        self.Xt = Xt\n",
    "        self.yt = yt\n",
    "        \n",
    "        df = pd.DataFrame(Xt)\n",
    "        \n",
    "        #limiting the nan values of a feature to 45% as model sees a degredation in performance beyond this threshold        \n",
    "        if not all(i< 0.45 for i in df.isna().mean().to_list()):\n",
    "            raise ValueError('Excessive proportion of NaN values detected. Percentage missing must be below 45%.')\n",
    "            \n",
    "            \n",
    "        df['y'] = yt\n",
    "        \n",
    "        self.dfs = data_splitter(df, 'y')\n",
    "        self.prior_probabilities= prior_probability_generator(df['y'])\n",
    "        self.feature_count = len(Xt[0])\n",
    "        \n",
    "        nan_std_mean_calculator()\n",
    "\n",
    "        \n",
    "    def nan_std_mean_calculator(self):\n",
    "            \n",
    "        #have std and mean for nan cleaned transposed arrays\n",
    "        for key, value in self.dfs.items():\n",
    "            self.std_mean_dict[key] = {}\n",
    "\n",
    "            for column in self.dfs[key]:\n",
    "                values = self.dfs[key][column].values\n",
    "                values = values[~np.isnan(values)]\n",
    "                std = np.std(values)\n",
    "                mean = np.mean(values)\n",
    "                self.std_mean_dict[key][column] = {\"std\" : std, \"mean\": mean}   \n",
    "                \n",
    "        print(self.std_mean_dict)\n",
    "        \n",
    "    def predict(self, X_array):\n",
    "        \n",
    "        def array_input_length_checker(input_array):\n",
    "            if len(X_array[0]) != self.feature_count:\n",
    "                raise ValueError('Predicted item length must be the same as fit training set')\n",
    "                \n",
    "        \n",
    "        #checking that input has same number of features as fit was trained on\n",
    "        array_input_length_checker(X_array)\n",
    "        \n",
    "        #checking that predict input array is all of same length\n",
    "        self.shape_checker(X_array)\n",
    "        \n",
    "         \n",
    "        \n",
    "        output_array = []\n",
    "        \n",
    "        for element in X_array:\n",
    "            output_array.append(self.predict_single(element))\n",
    "            \n",
    "        return output_array\n",
    "        \n",
    "\n",
    "    def predict_single(self, input_value):\n",
    "        \n",
    "        \n",
    "        def not_nan_indexes(input_list):\n",
    "            \"\"\"\n",
    "                Returns a list of indices where values are not \n",
    "            \"\"\"\n",
    "            not_nan = np.argwhere(~np.isnan(input_list)).tolist()\n",
    "            return [item for sublist in not_nan for item in sublist]\n",
    "        \n",
    "        def feature_conditional_probability_generator(x_value, feature, class_name):\n",
    "            \"\"\"\n",
    "                Calculates the conditional probability of a feature value based on the associated x feature value passed\n",
    "                Formula was sourced from assignement problem statement as requested\n",
    "                return value is float of the operation result\n",
    "\n",
    "            \"\"\"\n",
    "            \n",
    "            exponential_component = np.exp(-((x_value - self.std_mean_dict[class_name][feature]['mean']) ** 2 / (2 * self.std_mean_dict[class_name][feature]['std'] ** 2)))\n",
    "            return (1 / (np.sqrt(2 * np.pi) *  self.std_mean_dict[class_name][feature]['std']))*exponential_component\n",
    "        \n",
    "        \n",
    "        def single_class_conditionals_generator(x_value, class_name, non_nan_list):\n",
    "            \"\"\"\n",
    "                Generates a dictionary of the all conditional values for a single class and assignes them to a dictionary\n",
    "                Probabilities for each feature of the class are calculated using the feature_conditional_probability_generator() function \n",
    "                returns said dictionary\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            class_conditionals = {}\n",
    "            \n",
    "            #here instead of iterating over all columns in the dataframe, i can simple use a for element in non_nan list\n",
    "            #this will allow me to only calculate the conditionals on the non nan values present\n",
    "            \n",
    "            for element in non_nan_list:\n",
    "                class_conditionals[element] = feature_conditional_probability_generator(x_value[element], element, class_name)\n",
    "\n",
    "            return class_conditionals\n",
    "        \n",
    "        def all_class_conditional_dict_generator(x_value, non_nan_list):\n",
    "            \"\"\"\n",
    "                Generates a dictionary of all class probability values and assigns them to a dictionary\n",
    "                Probabilties for each class conditionals are calculated using the single_class_conditionals_generator() fucntion\n",
    "                return value is said dictionary\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            all_class_conditionals = {}\n",
    "\n",
    "            for class_name in self.dfs.keys():\n",
    "                all_class_conditionals[class_name] = single_class_conditionals_generator(x_value, class_name, non_nan_features)\n",
    "\n",
    "            return all_class_conditionals\n",
    "        \n",
    "        def class_probability_generator(class_name):\n",
    "            \"\"\"\n",
    "            Generates the probability of each target class for the given predict input\n",
    "            Class prior probabilities are sourced from the prior_probabilities dictionary generated from the fit() method\n",
    "            Class probabilities are generated via Naive baysienne by multiplying the class prior probability by the product of all class conditionals\n",
    "            return value is the class probability float value\n",
    "\n",
    "            \"\"\"\n",
    "            \n",
    "             \n",
    "            class_prior_prob = self.prior_probabilities[class_name]\n",
    "            \n",
    "            #as i designed my all_class_conditional generator to only create non-nan conditionals this remains constant\n",
    "            class_probability = class_prior_prob * (np.prod(list(all_class_conditionals[class_name].values())))\n",
    "\n",
    "            return class_probability\n",
    "        \n",
    "        def all_class_probability_generator():\n",
    "            \"\"\"\n",
    "            Generates a dictionary containing the class probabilties for each class of the target feature\n",
    "            Class probabilties are generated from the class_probability_generator() function\n",
    "            return value is said dictionary\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            class_probabilities = {}\n",
    "\n",
    "            for class_name in self.dfs.keys():\n",
    "                class_probabilities[class_name] = class_probability_generator(class_name)\n",
    "\n",
    "            return class_probabilities\n",
    "        \n",
    "        def class_tagger(normalized_probability_dict):\n",
    "            \"\"\"\n",
    "                Returns the key with max value from probability dict\n",
    "\n",
    "            \"\"\"\n",
    "            return max(normalized_probability_dict, key=normalized_probability_dict.get)\n",
    "        \n",
    "        \n",
    "        #input validation on predict value\n",
    "        if (np.count_nonzero(np.isnan(input_value))/len(input_value) == 1.0):\n",
    "            raise ValueError(\"Input must have at least one non-nan value\")\n",
    "            \n",
    "        #retrieving the index of non-nan values\n",
    "        non_nan_features = not_nan_indexes(input_value)\n",
    "        \n",
    "        #generating available conditional probability values for non-nan features\n",
    "        all_class_conditionals = all_class_conditional_dict_generator(input_value, non_nan_features)\n",
    "        \n",
    "        #obtaining highest probability class\n",
    "        predicted_probability = class_tagger(all_class_probability_generator())\n",
    "        \n",
    "        return predicted_probability   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dac4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "myGb = my_GB_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee2756",
   "metadata": {},
   "outputs": [],
   "source": [
    "myGb.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3000a51b",
   "metadata": {},
   "source": [
    "#### Single predict tests (Adelie and Gentoo) hand picked from from dataset for proof of concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357c4d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_test_adelie = np.array([np.nan, 18.7, 181.0, np.nan])\n",
    "myGb.predict_single(nan_test_adelie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b209708",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_test_gentoo = np.array([46.1, np.nan, 211.0, 4500.0])\n",
    "myGb.predict_single(nan_test_gentoo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e9edf9",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69401947",
   "metadata": {},
   "source": [
    "## Exploration of imputation outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de7d330",
   "metadata": {},
   "source": [
    "Comment: Here I am performing a simple exploration of the various imputation strategies.  Vizualizations of single imputation, knn and multivariate strategies are produced. Exploration of impact of imputation strategies on classifier performance is done in the next section and contrasted with performance of the custom Gaussian Bayes model which works in the presence of missing values.\n",
    "\n",
    "While not requiered as per assignement problem statement, I opted to keep this in as I found the exploration of the imputation values themselves to be of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60e64ee",
   "metadata": {},
   "source": [
    "#### 1st - Building single imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de1c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c2e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f9717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_imputer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf41a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean_imputed = mean_imputer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e5ff9e",
   "metadata": {},
   "source": [
    "#### 2nd - Building knn imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfda157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d897b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_imputer = KNNImputer(n_neighbors=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195077ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_knn_imputed = knn_imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a27d45f",
   "metadata": {},
   "source": [
    "#### 3rd - Building  Iterative Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b50c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6093ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_imputer = IterativeImputer(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c5182",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_imputer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f384ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_iter_imputed = iter_imputer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f6b154",
   "metadata": {},
   "source": [
    "#### Quick exploration of number of differences between iterative and mean imputer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7356c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_difference = []\n",
    "for i, (a_val, b_val) in enumerate(zip(X_mean_imputed, X_iter_imputed)):\n",
    "    for i, (nested_a, nested_b) in enumerate(zip(a_val, b_val)):\n",
    "        if nested_a!= nested_b:\n",
    "            mean_difference.append(abs(nested_a-nested_b))\n",
    "            print(f\"Mean_imputed val: {nested_a}, iter_imputed val:{nested_b}, Abs difference: {abs(nested_a-nested_b)}\")\n",
    "            \n",
    "print(f\"\\nNumber of differences: \", len(mean_difference), \", Mean difference: \", sum(mean_difference)/len(mean_difference))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e2c74f",
   "metadata": {},
   "source": [
    "Comment: Here we can see that iterative imputator has returned a variety of values for the missing values in penguins, let's explore this further. (Please note this was originally formatted with the 20% missing values on a single column, utilizing the updated 20% has changed the output of the cell and I have not been able to retrieve the original file)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104566b9",
   "metadata": {},
   "source": [
    "### Vizualizing multivariate imputator outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05b6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fad6b3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.hist(X_iter_imputed.T[3],alpha=0.5, label=\"iterative_imputation\")\n",
    "plt.hist(X_knn_imputed.T[3],alpha=0.5, label=\"knn_imputation\")\n",
    "\n",
    "plt.title(\"Comparaison of knn and iterative inputation value distributions\")\n",
    "plt.xlabel(\"Feature values\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec93b5c",
   "metadata": {},
   "source": [
    "Comment: The above plot demonstrates that these two forms of imputation do indeed produce slightly different results. However these do remain broadly similar.Further investigation into the impact on model output is requiered to determine potential significance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eff987",
   "metadata": {},
   "source": [
    "#### Reviewing basic dataframe staistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a40ae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_describe = pd.DataFrame(X_iter_imputed.T[3])\n",
    "df_describe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcaa1da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_describe2 = pd.DataFrame(X_knn_imputed.T[3])\n",
    "df_describe2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7623089",
   "metadata": {},
   "source": [
    "Comments: Interestingly the min and max values remain the sme, with minor differences in the mean, std and quartile spread. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dd051e",
   "metadata": {},
   "source": [
    "#### Exploration of K value on knn imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc09a52e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imputers = []\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "for i in range(1,51,5):\n",
    "    imputer = KNNImputer(n_neighbors=i)\n",
    "    imputed = imputer.fit_transform(X)\n",
    "    plt.hist(imputed.T[3],alpha=0.5, label=f\"{i}\")\n",
    "    \n",
    "plt.title(\"Comparaison of knn imputation value distributions while varying k\")\n",
    "plt.xlabel(\"Feature values\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678c01de",
   "metadata": {},
   "source": [
    "Comment: here we can see that varying the k value of the nearst neighbors output does lead to slight variations in the imputers output values as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d545a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exploration of multivariate imputation order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96bd0ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "imputation_values = []\n",
    "\n",
    "imputation_orders =[\"ascending\", \"descending\", \"roman\", \"arabic\", \"random\"]\n",
    "\n",
    "for value in imputation_orders:\n",
    "    imputer = IterativeImputer(random_state=0, imputation_order=value)\n",
    "    imputer.fit(X)\n",
    "    imputed = imputer.transform(X)\n",
    "    \n",
    "    imputation_values.append(imputed.T[3])\n",
    "    \n",
    "    plt.hist(imputed.T[3],alpha=0.5, label=f\"{value}\")\n",
    "    \n",
    "plt.title(\"Comparaison of multivarite imputation order on feature value distribution\")\n",
    "plt.xlabel(\"Feature values\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5683be8",
   "metadata": {},
   "source": [
    "Comment: Much more minor differences here in terms of the values output when compared to knn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a883db",
   "metadata": {},
   "source": [
    "#### Final section comments: \n",
    "\n",
    "Interesting how changes to parameters on both imputors does change values, naturally this would lead us to a grid search to understand how we can optimize these imputors as a way to maxmize our classifier accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90e2456",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8eeec5",
   "metadata": {},
   "source": [
    "# 2.1 Exploring the impact of imputation on classifier performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4093f8db",
   "metadata": {},
   "source": [
    "### Exploring the approx. 40% missing values penguins dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2547131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = pd.read_csv('PenguinsMV0.4.csv', index_col = 0,na_values = '?')\n",
    "print(penguins.shape)\n",
    "penguins.head()\n",
    "X = penguins.drop(columns=['species']).values\n",
    "y = penguins['species']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172bc687",
   "metadata": {},
   "source": [
    "#### Splitting our data into test/train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc0af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=420)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e29d12e",
   "metadata": {},
   "source": [
    "### Single imputation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccfe508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building pipeline\n",
    "singleImpPipe  = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', GaussianNB())])\n",
    "\n",
    "#creating parameters dict\n",
    "param_grid = {'imputer__strategy':[\"mean\", \"median\", \"most_frequent\", \"constant\"]}\n",
    "\n",
    "#creating gridsearch object\n",
    "single_gs = GridSearchCV(singleImpPipe,param_grid,cv=10, \n",
    "                      verbose = 1, n_jobs = -1)\n",
    "\n",
    "#fitting to data\n",
    "single_gs = single_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97857179",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c4bb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_gs = single_gs.predict(X_test)\n",
    "print(\"Accuracy: {0:4.2f}\".format(accuracy_score(y_test,y_pred_gs)))\n",
    "confusion_matrix(y_test, y_pred_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ef616b",
   "metadata": {},
   "source": [
    "Comment: Accuracy of 88 percent with the best imputer strategy being mean value imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bf0f0e",
   "metadata": {},
   "source": [
    "### Knn imputation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed796238",
   "metadata": {},
   "outputs": [],
   "source": [
    "kNNpipe  = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(missing_values = np.nan)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', GaussianNB())])\n",
    "\n",
    "param_grid = {'imputer__n_neighbors': [x for x in range(1,100, 3)],\n",
    "              'imputer__weights': ['uniform', 'distance']}\n",
    "\n",
    "knn_gs = GridSearchCV(kNNpipe,param_grid,cv=10, \n",
    "                      verbose = 1, n_jobs = -1)\n",
    "\n",
    "knn_gs = knn_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a95f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d24e75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_gs = knn_gs.predict(X_test)\n",
    "print(\"Accuracy: {0:4.2f}\".format(accuracy_score(y_test,y_pred_gs)))\n",
    "confusion_matrix(y_test, y_pred_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd73ca1",
   "metadata": {},
   "source": [
    "Comment: Slightly higher accuracy for the knn model at 89% with an 58 k value and uniform weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1c1085",
   "metadata": {},
   "source": [
    "### Multivariate pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ee679b",
   "metadata": {},
   "source": [
    "#### Defining a standard piepline\n",
    "\n",
    "Here simple defining a standard pipeline that will act as a reference for our pipeline gridsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1422d80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multiPipe  = Pipeline(steps=[\n",
    "    ('imputer', IterativeImputer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', GaussianNB())])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228b1224",
   "metadata": {},
   "source": [
    "#### Defining our imputer parameters for our grid_search\n",
    "\n",
    "Exploring the parameters for a single imputation strategu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83efb84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'imputer__imputation_order':[\"ascending\", \"descending\", \"roman\", \"arabic\", \"random\"], \n",
    "              'imputer__initial_strategy':[\"mean\", \"median\", \"most_frequent\", \"constant\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd82bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating out gridsearch object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33891a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "multi_gs = GridSearchCV(multiPipe,param_grid,cv=10, \n",
    "                      verbose = 1, n_jobs = -1)\n",
    "\n",
    "multi_gs = multi_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee861c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c649d003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_gs = multi_gs.predict(X_test)\n",
    "print(\"Accuracy: {0:4.2f}\".format(accuracy_score(y_test,y_pred_gs)))\n",
    "confusion_matrix(y_test, y_pred_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c07127",
   "metadata": {},
   "source": [
    "Comment: Lowest accuracy of the sklearn models tested. Model is experimental so cannot judge it's final value based on this test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcb85c4",
   "metadata": {},
   "source": [
    "#### Custom NB with explicit NaN consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20b30db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cNBpipe  = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', my_GB_3())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9de589",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'scaler__with_mean':[True, False]}\n",
    "\n",
    "cNB_gs = GridSearchCV(cNBpipe,param_grid, cv=10, \n",
    "                      verbose = 1, n_jobs = -1)\n",
    "\n",
    "cNB_gs = cNB_gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeef356",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_cNB_gs = cNB_gs.predict(X_test)\n",
    "print(\"Accuracy: {0:4.2f}\".format(accuracy_score(y_test,y_pred_cNB_gs)))\n",
    "confusion_matrix(y_test, y_pred_cNB_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f71efa",
   "metadata": {},
   "source": [
    "Comment: 85 percent accuracy, slightly lower than the single imputation model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f6a6f",
   "metadata": {},
   "source": [
    "### Exploring the approx. 20% missing values penguins dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33206aee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "penguins = pd.read_csv('PenguinsMV0.2.csv', index_col = 0,na_values = '?')\n",
    "print(penguins.shape)\n",
    "penguins.head()\n",
    "X = penguins.drop(columns=['species']).values\n",
    "y = penguins['species']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=420)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07635367",
   "metadata": {},
   "source": [
    "### Single imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df1a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building pipeline\n",
    "singleImpPipe  = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', GaussianNB())])\n",
    "\n",
    "#creating parameters dict\n",
    "param_grid = {'imputer__strategy':[\"mean\", \"median\", \"most_frequent\", \"constant\"]}\n",
    "\n",
    "#creating gridsearch object\n",
    "single_gs = GridSearchCV(singleImpPipe,param_grid,cv=10, \n",
    "                      verbose = 1, n_jobs = -1)\n",
    "\n",
    "#fitting to data\n",
    "single_gs = single_gs.fit(X_train, y_train)\n",
    "\n",
    "single_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c27bae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gs = single_gs.predict(X_test)\n",
    "print(\"Accuracy: {0:4.2f}\".format(accuracy_score(y_test,y_pred_gs)))\n",
    "confusion_matrix(y_test, y_pred_gs)\n",
    "\n",
    "single_accuracy_score_20 = accuracy_score(y_test,y_pred_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c587d7",
   "metadata": {},
   "source": [
    "Comment: Higher accuracy here with less missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6948d868",
   "metadata": {},
   "source": [
    "### Knn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfd4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kNNpipe  = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(missing_values = np.nan)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', GaussianNB())])\n",
    "\n",
    "param_grid = {'imputer__n_neighbors': [x for x in range(1,100, 3)],\n",
    "              'imputer__weights': ['uniform', 'distance']}\n",
    "\n",
    "knn_gs = GridSearchCV(kNNpipe,param_grid,cv=10, \n",
    "                      verbose = 1, n_jobs = -1)\n",
    "\n",
    "knn_gs = knn_gs.fit(X_train, y_train)\n",
    "\n",
    "knn_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc60270",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gs = knn_gs.predict(X_test)\n",
    "print(\"Accuracy: {0:4.2f}\".format(accuracy_score(y_test,y_pred_gs)))\n",
    "confusion_matrix(y_test, y_pred_gs)\n",
    "\n",
    "knn_accuracy_score_20 = accuracy_score(y_test,y_pred_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a150a6",
   "metadata": {},
   "source": [
    "Comment: Similar accuracy to the single imputation model model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e46efc",
   "metadata": {},
   "source": [
    "### Multivariate pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1337223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiPipe  = Pipeline(steps=[\n",
    "    ('imputer', IterativeImputer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', GaussianNB())])\n",
    "\n",
    "param_grid = {'imputer__imputation_order':[\"ascending\", \"descending\", \"roman\", \"arabic\", \"random\"], \n",
    "              'imputer__initial_strategy':[\"mean\", \"median\", \"most_frequent\", \"constant\"]}\n",
    "\n",
    "\n",
    "multi_gs = GridSearchCV(multiPipe,param_grid,cv=10, \n",
    "                      verbose = 1, n_jobs = -1)\n",
    "\n",
    "multi_gs = multi_gs.fit(X_train, y_train)\n",
    "\n",
    "multi_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d92c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gs = multi_gs.predict(X_test)\n",
    "print(\"Accuracy: {0:4.2f}\".format(accuracy_score(y_test,y_pred_gs)))\n",
    "confusion_matrix(y_test, y_pred_gs)\n",
    "\n",
    "iter_accuracy_score_20 = accuracy_score(y_test,y_pred_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd9f85",
   "metadata": {},
   "source": [
    "Comment: Interestingly, here the multivariate pipeline has outperformed the previous two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673eceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cNBpipe  = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', my_GB_3())])\n",
    "\n",
    "param_grid = {'scaler__with_mean':[True, False]}\n",
    "\n",
    "cNB_gs = GridSearchCV(cNBpipe,param_grid, cv=10, \n",
    "                      verbose = 1, n_jobs = -1)\n",
    "\n",
    "cNB_gs = cNB_gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660d9b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred_cNB_gs = cNB_gs.predict(X_test)\n",
    "print(\"Accuracy: {0:4.2f}\".format(accuracy_score(y_test,y_pred_cNB_gs)))\n",
    "confusion_matrix(y_test, y_pred_cNB_gs)\n",
    "\n",
    "custom_accuracy_score_20 = accuracy_score(y_test,y_pred_cNB_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce114d82",
   "metadata": {},
   "source": [
    "Comment: Similar accuracy to the multivariate mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f41d6b",
   "metadata": {},
   "source": [
    "#### Final section comments: \n",
    "\n",
    "It appears that the optimal strategy to employ to maxmimze classifier accuracy evolves as the proportion of missing data varies. For lower proportions of missing data, it appears that an iterative strategy or simply ignoring the nan values yeilds the best results, while at higher percentages of missing value simple imputation has outperformed all other models. \n",
    "\n",
    "This requires further investigation and discussion which are outline in the following section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fe3fcf",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ef2798",
   "metadata": {},
   "source": [
    "## Research and discussion section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe41517b",
   "metadata": {},
   "source": [
    "Naturally, due to the existance of multiple imputation models I expected there to be a variety of underlying reasons as to why and when different models yielded such results. I opted to include the following literature quote snippets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af2903b",
   "metadata": {},
   "source": [
    "#### Acock, Alan C. \"Working with missing values.\" Journal of Marriage and family 67.4 (2005): 1012-1028.\n",
    "\n",
    "Caution should be exercised due to the bias it can lead to with regards to estimates and hypothesis creation. It is important to understand the underlying dataset one is working with to take into account why the data is missing. There are various strategies that can be employed based on why the data is missing. \n",
    "\n",
    "Main missing data categories:\n",
    "\n",
    "Missing Completely at Random --> missing values are randomly distributed throughout the matrix\n",
    "\n",
    "Missing at Random --> missing data for a variable are MAR if the likelihood of missing data on the variable is not related to the participant's score on the variable, after controlling for other variables in the study\n",
    "\n",
    "NI missing values --> missing in ways that are neither MAR nor MCAR, but nevertheless are systematic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97edf74",
   "metadata": {},
   "source": [
    "#### Takeaway: \n",
    "Understanding the source of dataset as well as the distribution of missing values helps in understanding how that missing dataset should be interpreted and handled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc334dd",
   "metadata": {},
   "source": [
    "#### Kumar, S., 2020. 7 Ways to Handle Missing Values in Machine Learning. [online] Medium. Available at: <https://towardsdatascience.com/7-ways-to-handle-missing-values-in-machine-learning-1a6326adf79e> [Accessed 8 December 2021].\n",
    "\n",
    "Many ways to handle missing data there are serious drawbacks to most missing data handling strategies. Mean imputation  does not factor covariance. For small amounts of missing data rows can be deleted but does run risk of disgarding a lot of information if studying specific subset of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3deb75",
   "metadata": {},
   "source": [
    "#### Takeaway:\n",
    "Best strategy is highly contextual and should be critically appraised in terms of costs before implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e963a90c",
   "metadata": {},
   "source": [
    "#### COMP47350 - (Assoc Professor Georgiana Ifrim):\n",
    "    \n",
    "Indicated that approximately 40% of missing data is grounds for dropping a column as a best practice (will depend on target feature and other information contained in missing rows)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f32f1a1",
   "metadata": {},
   "source": [
    "#### Takeaway: \n",
    "\n",
    "Setting my limit on missing data to 45% for any given column in the fit section of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ffc464",
   "metadata": {},
   "source": [
    "#### Vizualization the distribution of missing data in our sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82854e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa364eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('PenguinsMV0.2.csv', index_col = 0,na_values = '?')\n",
    "sns.heatmap(df.isnull(), cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdd6bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('PenguinsMV0.4.csv', index_col = 0,na_values = '?')\n",
    "sns.heatmap(df.isnull(), cbar=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
